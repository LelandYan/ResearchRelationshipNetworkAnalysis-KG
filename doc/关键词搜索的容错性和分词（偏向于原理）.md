在搜索引擎中，例如谷歌、百度、必应等，都会对用户输入的关键词具有纠错和分词的功能。搜索引擎其实可以看成一个巨大数据库加上一个动态的网络爬虫，搜索引擎需要根据使用输入的关键词返回用户想要搜索的相关的内容。但是搜索引擎是无法保证使用者输入数据的正确性，例如单词的拼写是否正确、输入的格式是否规范等。当然搜索引擎可以通过返回没有该相关的数据或者返回给用户不想要的数据，但这样回影响用户的体验。所以现在的搜索引擎都会具有一定的纠错和分词功能。例如，如果用户输入了关键词"machine lear"，用户本来的意思是想要搜索“machine learning”，但是单词输入的时候发生了拼写错误，这里我么就需要使用我们的方法，基于贝叶斯的方法给出用户搜索最可能单词对原输入进行修正，当然这个修正的结果最后可能并不是用户所希望的，但是当我们的搜集的单词和数据已经足够大，可以以较好的效果进行预测修正。在例如如果用户输入的关键词是“deeplearning”，用户本来可能是想要搜索“deep learning”，但是单词输入的时候并不是十分的规范，导致我们在数据库中寻找的时候无法找到相关的数据，所以这里我们需要通过搜集的维基百科常用的词，对原字符串进行分割处理，也可以理解为分词，将"deeplearning"分割为“deep learning”。完成用户想要的搜索，得到相应的结果。下面我们将分别介绍关键词搜索的容错性和分词的具体原理。

关键词搜索的容错性：

我们的实现关键词搜索的容错性本质就是，选择和我们输入最为相似的拼写正确的单词。但是对于一个单词，我们没有办法确定“lates”该被纠正为“late”还是“latest”还是“latte”，所以我们这里使用基于概率论的判断，而不是基于规则的判断。我们试图找到在给定原始单词w下修正c，在所有可能的候选修正中，最大化c是预期修正的概率。
$$
\operatorname{argmax}_{c} P(c \mid w)
$$
然后将上式按照贝叶斯公式进行展开得到：
$$
\operatorname{argmax}_{c} P(w \mid c)P(c)/P(w)
$$
对于用户输入的任何单词都是可以的，所以P(w)的概率都是一样的，所以我们可以将上面公式进行简化：
$$
\operatorname{argmax}_{c} P(w \mid c)P(c)
$$
其中p(c)表示文章中出现一个正确的单词c的概率。这个概率值可以是由搜索引擎后台的搜集字典来进行确定。P(w |c)表示的是用户想要输入单词c的情况下输入成了w 的概率。

同上上面得到的公式，对于给定一个单词w，我们使用编辑距离的方式来枚举单词的所有可选方案。

单词的编辑距离可以看出使用了几次下面操作从一个词变到另一个词.

几次插入(在词中插入一个单字母), 

删除(删除一个单字母), 

交换(交换相邻两个字母), 

替换(把一个字母换成另一个)

最后我们需要计算所有的待选词（可以通过限制的编辑距离来挑选待选词）的概率值，然后对他们进行排序选择最大概率值就可以得到我们修正后的单词。

关键词搜索的分词：

我们的实现关键词搜索的分词本质就是，选择和我们输入连续单词字符串最为相似的分割后的单词列表。对于一个联系单词的字符串，例如“machinelearning”，我们需要通过我们搜集的一元单词的频率表来获得最大可能的分割后的单词列表，我们通过理“machinelearning” 可以得到“machine learning”的结果。

本文中我们使用基于英语维基百科一元频率的自然语言处理对连接的单词进行概率拆分。对输出的分布进行建模，近似是假设所有单词都是独立分布的。 然后根据所有单词的相对频率，可以合理地假设它们遵循齐普夫定律，即单词列表中排名为n的单词的概率约为1 /（n log N），其中N是词典中的单词数。固定模型后，可以使用动态规划来推断空间的位置。 最打可能的句子是使每个单词的概率乘积最大化的句子。通过动态规划可以计算出代替该句子的概率值，我们使用排序标准为概率倒数的对数，避免因为概率为0而出现的溢出。


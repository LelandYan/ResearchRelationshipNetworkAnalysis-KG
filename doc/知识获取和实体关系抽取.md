在本文章中，我们对于从清华的开源数据库Aminer获取的数据，为结构化数据，我们可以通过编写脚本来按实体与实体来区分其关系，从而得到我们的知识图谱构建所需要的三元组数据。而针对我们通过ACM文献的数据库中获得相关文章的信息，我们在通过爬虫爬取数据的时候，就编写规则，让其转化为结构化的数据，以便我们后期将其添加入我们从Aminer获取的数据中，完成对AMiner数据集的补充。最后针对pdf文章来说，我们则需要使用由华沙大学数学和计算模型跨学科中心开放科学中心用Java编写的CERMINE模型，对pdf文章进行关系提取，分别提取我们需要的三元组关系，例如文章标题、文章结构、文章引用、文章的发布地方、文章发表年限和合作者等。

1.针对ACM文献的数据库中获得相关文章的信息，我们需要使用scrapy爬虫框架，对爬取网址、字段、参数和myql数据库的定义。

Scrapy运行流程

- 引擎从调度器中取出一个链接(URL)用于接下来的抓取
- 引擎把URL封装成一个请求(Request)传给下载器
- 下载器把资源下载下来，并封装成应答包(Response)
- 爬虫解析Response
- 解析出实体（Item）,则交给实体管道进行进一步的处理
- 解析出的是链接（URL）,则把URL交给调度器等待抓取

首先使用mysql数据库定义ACM_Data表：

```sql
SET NAMES utf8mb4;
SET FOREIGN_KEY_CHECKS = 0;

-- ----------------------------
-- Table structure for ACM_Data
-- ----------------------------
DROP TABLE IF EXISTS `ACM_Data`;
CREATE TABLE `ACM_Data` (
  `p_id` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `title` varchar(511) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL,
  `authors` varchar(511) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL,
  `year` varchar(511) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL,
  `type` varchar(2047) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL,
  `subjects` varchar(255) DEFAULT NULL,
  `url` varchar(255) DEFAULT NULL,
  `abstract` varchar(4095) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL,
  `citation` int(10) DEFAULT NULL,
  PRIMARY KEY (`p_id`)
) ENGINE=InnoDB AUTO_INCREMENT=4376 DEFAULT CHARSET=utf8;

SET FOREIGN_KEY_CHECKS = 1;
```

首先在settings.py中设置我们的种子url，也就是我们爬取的初始url、使用的数据库mysql 的配置信息和我们使用的代理池构建。这里我们使用国内免费的ip代理，我们首先通过解析国内免费的ip代理网址，然后测试提取可使用的ip、端口、类型和相关的信息，并保存在acaSpider/proxy_list.txt中。

```python
# seed URL
ACM_URL = ["https://dl.acm.org/action/doSearch?filed"]
# Database
MYSQL_HOST = 'localhost'
MYSQL_DBNAME = 'papers'
MYSQL_USER = 'root'
MYSQL_PASSWORD = '2139637'
MYSQL_PORT = 3306
# Proxy
PROXY_LIST = 'acaSpider/proxy_list.txt'
PROXY_UPDATE_DELAY = 200
```

然后在scrapy中items.py的定义我们需要的实体的类型和名称：

```python
class AcaspiderItem(scrapy.Item):
    # define the fields for your item here like:
    title = scrapy.Field()
    authors = scrapy.Field()
    year = scrapy.Field()
    typex = scrapy.Field()
    subjects = scrapy.Field()
    url = scrapy.Field()
    abstract = scrapy.Field()
    citation = scrapy.Field()
```

最后是编写我们的爬虫逻辑，我们通过观察搜索的结果，首先提取我们需要的文章标题、文章作者、文章发表年限、文章类型、文章的领域、文章url、文章的摘要和文章被引用的数量所代表的xpath路径，这里我们通过搜索文章总共数量和我们使用ip代理过程中爬虫的延迟来限制爬虫进入的深度，当我们的爬虫已经爬取文章的数量已经超过我们搜索的全部结果的数量，就会停止向更深层次的爬取。当我们的爬虫使用代理ip的延迟大于了我们设置的阈值，我们也会停止向更深层次的爬取。直到我们url池中没有有效的url为止。通过获取字典中的数据存入mysql数据库。我们使用我们爬取的ACM文献数据库的数据，对我们的AMiner文献数据库进行补充，特别是针对文章的作者、摘要和日期进行补充。

2.针对从清华的开源数据库Aminer获取的结构化数据，AMiner-Paper.txt、AMiner-Author.txt、AMiner-Author2Paper.txt和AMiner-Coauthor.txt做不同的处理。

AMiner-Paper.txt的数据格式如下:

![](imgs\13.png)

该数据每行分别代表文章索引号、文章标题、文章作者（多作者用逗号分隔）、文章隶属地方（多地方用逗号分隔）、发表年限、投递期刊或者会议、文章索引对应文章索引（有多行，每一行都表示一个引用）和文章摘要。

对于AMiner-Paper.txt的数据转化为能导入neo4j的数据格式，我们通过python的读取该文件的每个词条信息，分别分离出实体和实体间关系，文章作者实体、投递期刊或者会议实体、文章和投递期刊或者会议关系、文章和文章被引用的数量和文章作者和文章的合作者关系。并保存成csv文件，分别命名为e_paper.csv、e_venue.csv、r_paper2venue.csv和r_citation.csv



AMiner-Author.txt的数据格式如下：

![](imgs\14.png)

该数据每行分别代表作者索引号、作者名称（多作者用逗号分隔）、作者隶属地方（多地方用逗号分隔）、作者发表文章数量、作者发表论文被引用的数量、作者的H-index值、与该作者的A-index相等的P-Index、与该作者的A-index不相等的P-Index和作者的研究兴趣（用逗号分隔）。

对于AMiner-Author.txt的数据转化为能导入neo4j的数据格式，我们通过python的读取该文件的每个词条信息，分别分离出实体和实体间关系，作者实体、作者隶属地方实体、作者感兴趣领域实体、作者和作者隶属机构关系和作者和作者感兴趣领域实体关系。并保存成csv文件，分别命名为e_author.csv、e_affiliation.csv、r_concept.csv、r_atuhor2affiliation.csv和r_author2concept.csv。



AMiner-Coauthor.txt的数据格式如下：

\#693728 1658348 2

该数据每行分别代表作者索引号、另一个作者索引号和他们之间的合作数量。

对于AMiner-Coauthor.txt的数据转化为能导入neo4j的数据格式，我们通过python的读取该文件的每个词条信息，分别分离出实体间关系，作者索引和作者索引之间关系。并保存成csv文件，命名为r_coauthor.csv。



AMiner-Author2Paper.txt的数据格式如下：

1	381617	1	1

该数据每行分别代表作者索引号、文章的索引号、作者在文章发表排名和是否具有关系。

对于AMiner-Author2Paper.txt的数据转化为能导入neo4j的数据格式，我们通过python的读取该文件的每个词条信息，分别分离出实体间关系，作者索引和文章索引之间关系。并保存成csv文件，命名为r_author2paper.csv。

3.对于格式为pdf的文章我们需要CERMINE模型，针对pdf文章首先进行的区分分割，然后对于分割后的各个区域，进行pdf转化为xml文本格式，在通过svm分类模型判断文本类型，最后通过nltk模型分词后进行实体的和实体关系的提取功能。我们只提取名词对应的实体和实体之间的关系，然后形成三元组，并保存为d3.js所需要的格式以便我们后期的知识图谱的渲染。

![](imgs\16.png)

```
[{'source': '', 'target': '', 'rela': '', 'type': ''}]
```

